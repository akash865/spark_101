{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark_101_DataManipulation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNhv1D9Le46fppX3ZJNBx1d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akash865/spark_101/blob/master/Spark_101_DataManipulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lihDEjAhyqca"
      },
      "source": [
        "# Spark 101 - Data Manipulation using Spark\n",
        "<hr size=\"5\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7k1hs7PCA6SA"
      },
      "source": [
        "> Notebook containing basic spark functions to get started with data analysis\n",
        "- Author: Akash Chandra\n",
        "- Comments: true\n",
        "- Categories: [Python, PySpark, Pandas]\n",
        "- Spark version: 3.0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9t08dFEeA9W"
      },
      "source": [
        "## **1. Running Spark in Colab**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0qXKDZMzLDc"
      },
      "source": [
        "Running spark codes in colab need some library imports. Please follow notebook to get started."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8dsXbdzzHDc"
      },
      "source": [
        "### 1.1 Initialize Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1UN3lvn1sKf"
      },
      "source": [
        "The below line of codes initializes spark. This installs Apache Spark 3.0.0, Java 8, and Findspark, a library that makes it easy for Python to find Spark. You might also need to refer to correct version while installing. Here I am using 3.0.3. Any update in the library location might lead to an error, please refer to the link below to find the stable version. [Link to library](https://downloads.apache.org/spark/)\n",
        "\n",
        "> Version: spark-3.0.3\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anLHk85lyQ5G"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.3-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzXlKXnGysC6"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop2.7\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJ6HCddh0rBI"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# sqlContext = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77fvmo9F0szJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e27fc3ee-19b5-40d3-9959-17c4130bec06"
      },
      "source": [
        "# Check if spark is initialized\n",
        "df = spark.sql('''select 'spark' as hello''')\n",
        "df.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+\n",
            "|hello|\n",
            "+-----+\n",
            "|spark|\n",
            "+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nnkg7pMe5tSy"
      },
      "source": [
        "If you are able to run the codes above, you are good to proceed. Let's start with ways to create dataframe in spark. We will also do some basic data manipulation. But let's start with importing useful libraries first. Then we will look at some useful dataframe methods.\n",
        "<br>\n",
        "<br>\n",
        "Please note that there are many functions available in `pyspark.sql.function` which will be helpful for data analysis and descriptive analytics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWbHuaova0PH"
      },
      "source": [
        "# Import spark libraries\n",
        "from pyspark.sql import Row, DataFrame\n",
        "from pyspark.sql.types import StringType, StructType, StructField, IntegerType\n",
        "from pyspark.sql.functions import col, expr, lit, substring, concat, concat_ws, when, coalesce\n",
        "from pyspark.sql import functions as F # for more sql functions\n",
        "from functools import reduce"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDLaLZjzzpWg"
      },
      "source": [
        "# Data manipulation using spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb6RizTuz6_f"
      },
      "source": [
        "Before we begin this section, let start with importing the data we have. We will use some basic functions usually needed when requiring analytics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ql57n4Iz4kX",
        "outputId": "0440d4cb-04e8-494c-943c-e645f44b1c74"
      },
      "source": [
        "# Using file uploaded to github\n",
        "from pyspark import SparkFiles\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/akash865/spark_101/master/banklist.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "df = spark.read.options(header=\"true\", inferschema = \"true\", delimiter=\",\").csv(\"file://\"+SparkFiles.get(\"banklist.csv\"))\n",
        "\n",
        "print('df.count  :', df.count())\n",
        "print('df.col ct :', len(df.columns))\n",
        "print('df.columns:', df.columns)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df.count  : 561\n",
            "df.col ct : 6\n",
            "df.columns: ['Bank Name', 'City', 'ST', 'CERT', 'Acquiring Institution', 'Closing Date']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lviT9J99TMmr"
      },
      "source": [
        "### **3. Using SQL in spark**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l11ahwlql8yM"
      },
      "source": [
        "Often there is a need to use SQL for data manipulation. Though spark provides nearly all functions that we use in SQL, it is often easier to use SQL because of its familiarity. Please do note that it is best practice to avoid spaces in column names (in the example below). We will also learn spark functions/methods that will help as we go along. We can start with schema details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jce3V6YyT_tm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62eac753-3e92-41e0-9f05-820c0d9aaf97"
      },
      "source": [
        "df.registerTempTable(\"temp_tb\")\n",
        "\n",
        "df_check = spark.sql('''select `Bank Name`, City, `Closing Date` from temp_tb''')\n",
        "df_check.show(4, truncate=False)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------------+-------------+------------+\n",
            "|Bank Name                       |City         |Closing Date|\n",
            "+--------------------------------+-------------+------------+\n",
            "|The First State Bank            |Barboursville|3-Apr-20    |\n",
            "|Ericson State Bank              |Ericson      |14-Feb-20   |\n",
            "|City National Bank of New Jersey|Newark       |1-Nov-19    |\n",
            "|Resolute Bank                   |Maumee       |25-Oct-19   |\n",
            "+--------------------------------+-------------+------------+\n",
            "only showing top 4 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_h4nezoVoMP"
      },
      "source": [
        "## **4 Dataframe Basic Operations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLoqnwVgtucp"
      },
      "source": [
        "### 4.1 Describe dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9DfA6Lnt5gC"
      },
      "source": [
        "Describe is a useful method which performs `count`, `mean`, `stddev`, `min` and `max` on all columns. We could limit our variables by passing in column name(s) in `describe`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJSOwS4Eturw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bd89042-0220-45b4-9f3e-f281fb8bb401"
      },
      "source": [
        "df.describe().show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+--------------------+-------+----+-----------------+---------------------+------------+\n",
            "|summary|           Bank Name|   City|  ST|             CERT|Acquiring Institution|Closing Date|\n",
            "+-------+--------------------+-------+----+-----------------+---------------------+------------+\n",
            "|  count|                 561|    561| 561|              561|                  561|         561|\n",
            "|   mean|                null|   null|null|31685.68449197861|                 null|        null|\n",
            "| stddev|                null|   null|null|16446.65659309965|                 null|        null|\n",
            "|    min|1st American Stat...|Acworth|  AL|               91|      1st United Bank|    1-Aug-08|\n",
            "|    max|               ebank|Wyoming|  WY|            58701|  Your Community Bank|    9-Sep-11|\n",
            "+-------+--------------------+-------+----+-----------------+---------------------+------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwVXN5fZn3Jz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e7a02d5-6e87-4b6e-cb6c-05737c36e896"
      },
      "source": [
        "df.describe('City', 'ST').show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-------+----+\n",
            "|summary|   City|  ST|\n",
            "+-------+-------+----+\n",
            "|  count|    561| 561|\n",
            "|   mean|   null|null|\n",
            "| stddev|   null|null|\n",
            "|    min|Acworth|  AL|\n",
            "|    max|Wyoming|  WY|\n",
            "+-------+-------+----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDht33CnhVL5"
      },
      "source": [
        "### 4.2 Counts, Columns and Schema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7M66kJr2fg6"
      },
      "source": [
        "Prior to data processing, it is imperative to look at `counts`, `columns` and `data types`. This is easily done below. We could look at the string and numerical columns using `dtypes`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O63DzXi1VoaN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a6f9eec-f711-443c-f9ac-fb9c2a8c29ca"
      },
      "source": [
        "print('df.count\t\t:', df.count())\n",
        "print('df.columns\t:', df.columns)\n",
        "print('df dtypes\t:', df.dtypes)\n",
        "print('df schema 1:', df.schema)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df.count\t\t: 561\n",
            "df.columns\t: ['Bank Name', 'City', 'ST', 'CERT', 'Acquiring Institution', 'Closing Date']\n",
            "df dtypes\t: [('Bank Name', 'string'), ('City', 'string'), ('ST', 'string'), ('CERT', 'int'), ('Acquiring Institution', 'string'), ('Closing Date', 'string')]\n",
            "df schema 1: StructType(List(StructField(Bank Name,StringType,true),StructField(City,StringType,true),StructField(ST,StringType,true),StructField(CERT,IntegerType,true),StructField(Acquiring Institution,StringType,true),StructField(Closing Date,StringType,true)))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeG5OcIq1vfg",
        "outputId": "9a5a6a7b-ab86-45c7-e356-ee8440970d44"
      },
      "source": [
        "print('df schema 1:')\n",
        "df.printSchema()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df schema 1:\n",
            "root\n",
            " |-- Bank Name: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- ST: string (nullable = true)\n",
            " |-- CERT: integer (nullable = true)\n",
            " |-- Acquiring Institution: string (nullable = true)\n",
            " |-- Closing Date: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSHkkGVBhd-F"
      },
      "source": [
        "### 4.3 Remove Duplicates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLoBe-Hl14_G"
      },
      "source": [
        "When working with data from other sources, it is often required to clean them before processing further. `dropDuplicates` remove all instances of rows with duplicates. Do note that the 'complete' duplicates rows are removed and it doesn't take into account the duplicate IDs. Removing duplicate IDs require the use of `window` function, we will visit later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcqzJ65IV0Fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65b5f28b-c316-4979-d443-29b7c1584f3b"
      },
      "source": [
        "df = df.dropDuplicates()\n",
        "print('df.count\t\t:', df.count())\n",
        "print('df.columns\t:', df.columns)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df.count\t\t: 561\n",
            "df.columns\t: ['Bank Name', 'City', 'ST', 'CERT', 'Acquiring Institution', 'Closing Date']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olIfm2KIicOn"
      },
      "source": [
        "### 4.4 Select specific columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGeZy2Vb3IEa"
      },
      "source": [
        "Specific columns can be stored in a dataframe using `select` statement. It accepts list as an argument for a list of specific columns/field."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wBMsReAWSUF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "410208db-22a6-44da-c17a-07f0e8b936c0"
      },
      "source": [
        "df2 = df.select(*['Bank Name', 'City'])\n",
        "df2.show(2)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+\n",
            "|           Bank Name|    City|\n",
            "+--------------------+--------+\n",
            "| First Bank of Idaho| Ketchum|\n",
            "|Amcore Bank, Nati...|Rockford|\n",
            "+--------------------+--------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMvFVdP_oK7W"
      },
      "source": [
        "### 4.5 Select multiple columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaiTNIJhotbA"
      },
      "source": [
        "In case we have a list of columns we need to drop, alternatively we could remove multiple columns from dataframe. So, we are using list operations in python to get the variable list subset. And then using `select` method creates a dataframe with specific columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS3tpP6qoLVu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e01f247-f2b8-4434-852f-cce91df9bbe2"
      },
      "source": [
        "col_l = list(set(df.columns)  - {'CERT','ST'})\n",
        "df2 = df.select(*col_l)\n",
        "df2.show(2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------------+--------------------+--------+------------+\n",
            "|Acquiring Institution|           Bank Name|    City|Closing Date|\n",
            "+---------------------+--------------------+--------+------------+\n",
            "|      U.S. Bank, N.A.| First Bank of Idaho| Ketchum|   24-Apr-09|\n",
            "|          Harris N.A.|Amcore Bank, Nati...|Rockford|   23-Apr-10|\n",
            "+---------------------+--------------------+--------+------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zfc5uWaoi_R2"
      },
      "source": [
        "### 4.6 Rename columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opk9G1Xyju0e"
      },
      "source": [
        "We will rename multiple columns in the example below. I found it annoying to look at spaces in field names. So let's start with removing spaces from the columns using `withColumnRenamed`. The first argument is the existing column and the 2nd argument is the new column name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdOkLYzlisvy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98ce5c55-464e-4981-edce-6ba6ea5829f3"
      },
      "source": [
        "df2 = df \\\n",
        "  .withColumnRenamed('Bank Name'            , 'bank_name') \\\n",
        "  .withColumnRenamed('Acquiring Institution', 'acq_institution') \\\n",
        "  .withColumnRenamed('Closing Date'         , 'closing_date') \\\n",
        "  .withColumnRenamed('ST'                   , 'state') \\\n",
        "  .withColumnRenamed('CERT'                 , 'cert') #\\\n",
        "\n",
        "df2.show(2)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+-----+-----+---------------+------------+\n",
            "|           bank_name|    City|state| cert|acq_institution|closing_date|\n",
            "+--------------------+--------+-----+-----+---------------+------------+\n",
            "| First Bank of Idaho| Ketchum|   ID|34396|U.S. Bank, N.A.|   24-Apr-09|\n",
            "|Amcore Bank, Nati...|Rockford|   IL| 3735|    Harris N.A.|   23-Apr-10|\n",
            "+--------------------+--------+-----+-----+---------------+------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veAUD18tl6oG"
      },
      "source": [
        "### 4.7 Rename columns using loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZaQTHwOXGo5"
      },
      "source": [
        "For renaming multiple columns, we could use loop to help us perform similar operations like case change, replace characters. In the below example, we are replacing all spaces with `_`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gnKDBPjjpka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "169afeab-7873-4ccd-9214-3de570f005c8"
      },
      "source": [
        "rename_expr = [col(column).alias(column.replace(' ', '_')) for column in df.columns]\n",
        "\n",
        "df2 = df.select(*rename_expr)\n",
        "df2.show(2)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+---+-----+---------------------+------------+\n",
            "|           Bank_Name|    City| ST| CERT|Acquiring_Institution|Closing_Date|\n",
            "+--------------------+--------+---+-----+---------------------+------------+\n",
            "| First Bank of Idaho| Ketchum| ID|34396|      U.S. Bank, N.A.|   24-Apr-09|\n",
            "|Amcore Bank, Nati...|Rockford| IL| 3735|          Harris N.A.|   23-Apr-10|\n",
            "+--------------------+--------+---+-----+---------------------+------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "momxuupJnqPd"
      },
      "source": [
        "### 4.8 Add columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L-DTkoAn_KP"
      },
      "source": [
        "In the below example, we will copy the column state using `col` function. PySpark SQL libraries contains many functions which we will be using through. Note that `col` is one such function which returns column values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDKwdHg5l_nm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35813a55-5ded-4f1a-bae8-81e6c86b6784"
      },
      "source": [
        "df2 = df.withColumn('state', col('ST'))\n",
        "df2.show(2)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+---+-----+---------------------+------------+-----+\n",
            "|           Bank Name|    City| ST| CERT|Acquiring Institution|Closing Date|state|\n",
            "+--------------------+--------+---+-----+---------------------+------------+-----+\n",
            "| First Bank of Idaho| Ketchum| ID|34396|      U.S. Bank, N.A.|   24-Apr-09|   ID|\n",
            "|Amcore Bank, Nati...|Rockford| IL| 3735|          Harris N.A.|   23-Apr-10|   IL|\n",
            "+--------------------+--------+---+-----+---------------------+------------+-----+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjkXUUpvou7B"
      },
      "source": [
        "### 4.9 Add constant column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOyNmNHUo8s0"
      },
      "source": [
        "What if we want to add a column with same value throughout. Say in this case, we want to add country - 'US' as a new column. `lit` helps us 'linear transform' this value to all rows in the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-gSou1GnpBd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d496e5a-a407-43e5-bf74-93b8b17c2b1f"
      },
      "source": [
        "df2 = df.withColumn('country', lit('US'))\n",
        "df2.show(2)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+---+-----+---------------------+------------+-------+\n",
            "|           Bank Name|    City| ST| CERT|Acquiring Institution|Closing Date|country|\n",
            "+--------------------+--------+---+-----+---------------------+------------+-------+\n",
            "| First Bank of Idaho| Ketchum| ID|34396|      U.S. Bank, N.A.|   24-Apr-09|     US|\n",
            "|Amcore Bank, Nati...|Rockford| IL| 3735|          Harris N.A.|   23-Apr-10|     US|\n",
            "+--------------------+--------+---+-----+---------------------+------------+-------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx7fAtmqZxSH"
      },
      "source": [
        "### 4.10 Drop columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzCpCPaq6hZx"
      },
      "source": [
        "Instead of selecting specific columns, we could also `drop` columns not required for analysis. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAKuWe7nZxmc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "961d9054-c8f0-4acc-b8bc-8e1c1e35a371"
      },
      "source": [
        "df2 = df.drop('CERT')\n",
        "df2.show(2)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+---+---------------------+------------+\n",
            "|           Bank Name|    City| ST|Acquiring Institution|Closing Date|\n",
            "+--------------------+--------+---+---------------------+------------+\n",
            "| First Bank of Idaho| Ketchum| ID|      U.S. Bank, N.A.|   24-Apr-09|\n",
            "|Amcore Bank, Nati...|Rockford| IL|          Harris N.A.|   23-Apr-10|\n",
            "+--------------------+--------+---+---------------------+------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG7tuEZpaGll"
      },
      "source": [
        "### 4.11 Drop multiple columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGLxvJ1baUww"
      },
      "source": [
        "As with `select`, dropping multipl columns is also simple. All we have to do is pass a list of columns using `*` to remove. Below are 2 ways to doing the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56E1uJPQpZJH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "418c2a69-ad9e-4c40-a941-8a8e9f18766f"
      },
      "source": [
        "df2 = df.drop(*['CERT','ST'])\n",
        "df2.show(2)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+---------------------+------------+\n",
            "|           Bank Name|    City|Acquiring Institution|Closing Date|\n",
            "+--------------------+--------+---------------------+------------+\n",
            "| First Bank of Idaho| Ketchum|      U.S. Bank, N.A.|   24-Apr-09|\n",
            "|Amcore Bank, Nati...|Rockford|          Harris N.A.|   23-Apr-10|\n",
            "+--------------------+--------+---------------------+------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuVEE3SqaFvC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abc62434-0ec0-4136-ef03-4066d5300edf"
      },
      "source": [
        "df2 = reduce(DataFrame.drop, ['CERT','ST'], df)\n",
        "df2.show(2)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+---------------------+------------+\n",
            "|           Bank Name|    City|Acquiring Institution|Closing Date|\n",
            "+--------------------+--------+---------------------+------------+\n",
            "| First Bank of Idaho| Ketchum|      U.S. Bank, N.A.|   24-Apr-09|\n",
            "|Amcore Bank, Nati...|Rockford|          Harris N.A.|   23-Apr-10|\n",
            "+--------------------+--------+---------------------+------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhuwX3AwbRiu"
      },
      "source": [
        "### 4.12 Filter data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r68C7hXnpzwq"
      },
      "source": [
        "When looking at data tables, we are required for 'cuts and slices' to get insights. Below we have examples to filter data using `>`, `<`, `==`, `between` and `isin`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeA5BOpJbD5O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b7ac410-088d-4673-fe94-71007c942524"
      },
      "source": [
        "# Equal to values\n",
        "df2 = df.where(df['ST'] == 'NE')\n",
        "\n",
        "# Between values\n",
        "df3 = df.where(df['CERT'].between('1000','2000'))\n",
        "\n",
        "# Is inside multiple values\n",
        "df4 = df.where(df['ST'].isin('NE','IL'))\n",
        "\n",
        "print('df.count  :', df.count())\n",
        "print('df2.count :', df2.count())\n",
        "print('df3.count :', df3.count())\n",
        "print('df4.count :', df4.count())\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df.count  : 561\n",
            "df2.count : 4\n",
            "df3.count : 9\n",
            "df4.count : 73\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0fL-hmR7XAU",
        "outputId": "881d72d8-3ac5-4758-fb82-ca119e5592e2"
      },
      "source": [
        "# Equal to values\n",
        "df2 = df.where(df['ST'] == 'NE')\n",
        "\n",
        "print('\\ndf2 sample below')\n",
        "df2.show(2)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "df2 sample below\n",
            "+-------------------+---------+---+-----+---------------------+------------+\n",
            "|          Bank Name|     City| ST| CERT|Acquiring Institution|Closing Date|\n",
            "+-------------------+---------+---+-----+---------------------+------------+\n",
            "|       TierOne Bank|  Lincoln| NE|29341|   Great Western Bank|    4-Jun-10|\n",
            "|Sherman County Bank|Loup City| NE| 5431|        Heritage Bank|   13-Feb-09|\n",
            "+-------------------+---------+---+-----+---------------------+------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvyiT6Virnk0"
      },
      "source": [
        "### 4.13 Filter data using logical operators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9vc-eJgrs8r"
      },
      "source": [
        "If we need to filter using multiple conditions, we could use logical operators. Please note the logical operators here, `AND`:`&`, `OR`:`|` and `NOT`:`!`. In the given dataframe, we have only 1 failed bank in state of 'NE' and city of 'Ericson'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2jx8gV3q70-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c592a14e-d72e-4414-843f-685b1a5abbc8"
      },
      "source": [
        "df2 = df.where((df['ST'] == 'NE') & (df['City'] == 'Ericson'))\n",
        "df2.show(3)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------+-------+---+-----+---------------------+------------+\n",
            "|         Bank Name|   City| ST| CERT|Acquiring Institution|Closing Date|\n",
            "+------------------+-------+---+-----+---------------------+------------+\n",
            "|Ericson State Bank|Ericson| NE|18265| Farmers and Merch...|   14-Feb-20|\n",
            "+------------------+-------+---+-----+---------------------+------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bImSbNBFcsNl"
      },
      "source": [
        "### 4.14 Cast datatypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX7goybEcsNn"
      },
      "source": [
        "Often we would need to change the datatype for a particular variable. When we create dataframe using text files, there is a possibility thaof datatype mismatch, due to some errors/missing in data. We could cast this to a different datatype as needed. We will deal with datetypes later on. Please note the two methods to cast datatype as string below. We could also use same variable name in the tranformed variable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kUFFW4FcsNp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60b74709-2fe0-4087-afff-37edd7ef45ed"
      },
      "source": [
        "print('='*50)\n",
        "print('Pre cast')\n",
        "print(df.printSchema())\n",
        "\n",
        "df2 = df \\\n",
        ".withColumn('CERT_str1', df['CERT'].cast('string')) \\\n",
        ".withColumn('CERT_str2', df['CERT'].cast(StringType())) #\\\n",
        "\n",
        "print('='*50)\n",
        "print('Post cast')\n",
        "print(df2.printSchema())\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Pre cast\n",
            "root\n",
            " |-- Bank Name: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- ST: string (nullable = true)\n",
            " |-- CERT: integer (nullable = true)\n",
            " |-- Acquiring Institution: string (nullable = true)\n",
            " |-- Closing Date: string (nullable = true)\n",
            "\n",
            "None\n",
            "==================================================\n",
            "Post cast\n",
            "root\n",
            " |-- Bank Name: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- ST: string (nullable = true)\n",
            " |-- CERT: integer (nullable = true)\n",
            " |-- Acquiring Institution: string (nullable = true)\n",
            " |-- Closing Date: string (nullable = true)\n",
            " |-- CERT_str1: string (nullable = true)\n",
            " |-- CERT_str2: string (nullable = true)\n",
            "\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lj8Vbx0gYqQ-"
      },
      "source": [
        "### 4.15 Coalesce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCznZIdnYqN3"
      },
      "source": [
        "`Coalesce` is a word which I frequently started using after learning SQL. In some cases, we are required to replace some columns containing nulls by a constant or maybe another column in the same data. We could use `CASE` statements to priortize replacing NULLs or using `coalesce` comes in handy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJ-Z7Z0FYsRS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbb0c08b-043b-4661-feac-0af20f09f029"
      },
      "source": [
        "df = spark.createDataFrame(\n",
        "    [\n",
        "      ['a1', 7, 5],\n",
        "      ['a2', 5, 11],\n",
        "      ['a3', None, 42],\n",
        "      ['a4', 10, 15]\n",
        "    ], # Data rows\n",
        "    ['user', 'day_1', 'day_2'] # Column names\n",
        ")\n",
        "\n",
        "df.show()\n",
        "\n",
        "print('='*50)\n",
        "print('Using Coalesce')\n",
        "df2 = df.withColumn('final_day', coalesce(col('day_1'), col('day_2'), lit(0))) # takes the first non-null value\n",
        "df2.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+-----+\n",
            "|user|day_1|day_2|\n",
            "+----+-----+-----+\n",
            "|  a1|    7|    5|\n",
            "|  a2|    5|   11|\n",
            "|  a3| null|   42|\n",
            "|  a4|   10|   15|\n",
            "+----+-----+-----+\n",
            "\n",
            "==================================================\n",
            "Using Coalesce\n",
            "+----+-----+-----+---------+\n",
            "|user|day_1|day_2|final_day|\n",
            "+----+-----+-----+---------+\n",
            "|  a1|    7|    5|        7|\n",
            "|  a2|    5|   11|        5|\n",
            "|  a3| null|   42|       42|\n",
            "|  a4|   10|   15|       10|\n",
            "+----+-----+-----+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOAWTJcIVLrs"
      },
      "source": [
        "### 4.16 Replace values in dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2hmf7qfVLpS"
      },
      "source": [
        "Replacing all values in dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJkLrgi-VMqM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d713a3dd-53cb-4151-a722-847ceb4616d5"
      },
      "source": [
        "# Pre replace\n",
        "df.show(2)\n",
        "\n",
        "# Post replace\n",
        "print('Replace 7 in the above dataframe with 17 at all instances')\n",
        "df.na.replace(7,17).show(2)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+-----+\n",
            "|user|day_1|day_2|\n",
            "+----+-----+-----+\n",
            "|  a1|    7|    5|\n",
            "|  a2|    5|   11|\n",
            "+----+-----+-----+\n",
            "only showing top 2 rows\n",
            "\n",
            "Replace 7 in the above dataframe with 17 at all instances\n",
            "+----+-----+-----+\n",
            "|user|day_1|day_2|\n",
            "+----+-----+-----+\n",
            "|  a1|   17|    5|\n",
            "|  a2|    5|   11|\n",
            "+----+-----+-----+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "658R7Nz6Xpa_"
      },
      "source": [
        "### 4.17 Sort values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hNCHNRvXpVN"
      },
      "source": [
        "Sorting in dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qD0bV-nAXse2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1584217b-bbdc-4d18-dbba-b4b419744750"
      },
      "source": [
        "# Unsorted\n",
        "print('Unsorted dataframe')\n",
        "df.show(5)\n",
        "\n",
        "# Default - ascending\n",
        "print('Post ascending sorted day_1')\n",
        "df.sort('day_1').show(5)\n",
        "\n",
        "# Descending sort\n",
        "print('Post descending sorted day_1')\n",
        "df.sort(col('day_1').desc()).show(5)\n",
        "## ANOTHER WAY TO WRITE THE STATEMENT IS ==> df.sort('ST',ascending=False).show(2)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unsorted dataframe\n",
            "+----+-----+-----+\n",
            "|user|day_1|day_2|\n",
            "+----+-----+-----+\n",
            "|  a1|    7|    5|\n",
            "|  a2|    5|   11|\n",
            "|  a3| null|   42|\n",
            "|  a4|   10|   15|\n",
            "+----+-----+-----+\n",
            "\n",
            "Post ascending sorted day_1\n",
            "+----+-----+-----+\n",
            "|user|day_1|day_2|\n",
            "+----+-----+-----+\n",
            "|  a3| null|   42|\n",
            "|  a2|    5|   11|\n",
            "|  a1|    7|    5|\n",
            "|  a4|   10|   15|\n",
            "+----+-----+-----+\n",
            "\n",
            "Post descending sorted day_1\n",
            "+----+-----+-----+\n",
            "|user|day_1|day_2|\n",
            "+----+-----+-----+\n",
            "|  a4|   10|   15|\n",
            "|  a1|    7|    5|\n",
            "|  a2|    5|   11|\n",
            "|  a3| null|   42|\n",
            "+----+-----+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eavk4KIfuuFe"
      },
      "source": [
        ""
      ],
      "execution_count": 28,
      "outputs": []
    }
  ]
}